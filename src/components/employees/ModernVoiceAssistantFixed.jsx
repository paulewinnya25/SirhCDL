import React, { useState, useEffect, useRef, useCallback } from 'react';
import { useNavigate } from 'react-router-dom';
import { RH_KNOWLEDGE_BASE, AGENT_UTILS } from '../../data/rhKnowledgeBase';
import elevenLabsService from '../../services/elevenLabsService';
import aiService from '../../services/aiService';
import './ModernVoiceAssistant.css';

const ModernVoiceAssistantFixed = ({ user, onClose }) => {
  const navigate = useNavigate();
  const [isListening, setIsListening] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [agentStatus, setAgentStatus] = useState('ready');
  const [elevenLabsStatus, setElevenLabsStatus] = useState('checking');
  const [voiceEnabled, setVoiceEnabled] = useState(true);
  const [currentQuery, setCurrentQuery] = useState('');
  const [responseText, setResponseText] = useState('');
  const [showWaveform, setShowWaveform] = useState(false);
  const [waveformData, setWaveformData] = useState([]);
  const [chatMessages, setChatMessages] = useState([]);
  const [inputText, setInputText] = useState('');
  const [isTyping, setIsTyping] = useState(false);
  const [chatMode, setChatMode] = useState('voice');
  const [showAIPanel, setShowAIPanel] = useState(false);
  
  const recognitionRef = useRef(null);
  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const animationFrameRef = useRef(null);

  // Initialiser l'assistant moderne
  const initializeAssistant = useCallback(async () => {
    try {
      console.log('ğŸš€ Initialisation de l\'assistant vocal...');
      
      // VÃ©rifier ElevenLabs
      const configResult = await elevenLabsService.checkConfiguration();
      if (configResult.success) {
        setElevenLabsStatus('connected');
        console.log('ğŸš€ Assistant moderne connectÃ© Ã  ElevenLabs');
      } else {
        setElevenLabsStatus('error');
        console.warn('âš ï¸ ElevenLabs non disponible, mode local activÃ©');
      }

      // Configuration de la reconnaissance vocale CORRIGÃ‰E
      if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        recognitionRef.current = new SpeechRecognition();
        
        // Configuration SIMPLIFIÃ‰E pour Ã©viter les conflits
        recognitionRef.current.continuous = false;  // âŒ CHANGÃ‰ : false au lieu de true
        recognitionRef.current.interimResults = false;  // âŒ CHANGÃ‰ : false au lieu de true
        recognitionRef.current.lang = 'fr-FR';
        recognitionRef.current.maxAlternatives = 1;  // âŒ CHANGÃ‰ : 1 au lieu de 3

        recognitionRef.current.onstart = () => {
          console.log('âœ… Ã‰coute dÃ©marrÃ©e');
          setIsListening(true);
          setAgentStatus('listening');
          setShowWaveform(true);
          startWaveformAnimation();
        };

        recognitionRef.current.onresult = (event) => {
          console.log('ğŸ“ RÃ©sultat reÃ§u:', event);
          
          // Traitement SIMPLIFIÃ‰ des rÃ©sultats
          const transcript = event.results[0][0].transcript;
          console.log('ğŸ—£ï¸ Transcription:', transcript);
          
          setCurrentQuery(transcript);
          
          // Traiter immÃ©diatement la commande vocale
          handleVoiceCommand(transcript);
          
          // IMPORTANT : Ne pas arrÃªter l'Ã©coute ici
          // L'assistant continuera d'Ã©couter aprÃ¨s avoir traitÃ© la commande
        };

        recognitionRef.current.onerror = (event) => {
          console.error('âŒ Erreur de reconnaissance vocale:', event.error);
          
          // Gestion d'erreur AMÃ‰LIORÃ‰E
          if (event.error === 'no-speech') {
            console.log('ğŸ”„ Aucune parole dÃ©tectÃ©e, redÃ©marrage de l\'Ã©coute...');
            // RedÃ©marrer l'Ã©coute aprÃ¨s un dÃ©lai PLUS LONG et seulement si pas dÃ©jÃ  en cours
            setTimeout(() => {
              if (agentStatus !== 'error' && !isSpeaking && !isListening) {
                startListening();
              }
            }, 3000); // âœ… CHANGÃ‰ : 3 secondes au lieu de 1
          } else if (event.error === 'audio-capture') {
            console.error('âŒ ProblÃ¨me de capture audio');
            setAgentStatus('error');
          } else if (event.error === 'network') {
            console.error('âŒ Erreur rÃ©seau');
            setAgentStatus('error');
          } else {
            console.error('âŒ Erreur inconnue:', event.error);
            setAgentStatus('error');
          }
        };

        recognitionRef.current.onend = () => {
          console.log('ğŸ›‘ Ã‰coute terminÃ©e');
          setIsListening(false);
          setShowWaveform(false);
          stopWaveformAnimation();
          
          // RedÃ©marrer automatiquement l'Ã©coute APRÃˆS un dÃ©lai PLUS LONG et seulement si pas dÃ©jÃ  en cours
          if (agentStatus !== 'error' && !isSpeaking && !isListening) {
            setTimeout(() => {
              console.log('ğŸ”„ RedÃ©marrage automatique de l\'Ã©coute...');
              startListening();
            }, 5000); // âœ… CHANGÃ‰ : 5 secondes au lieu de 2
          }
        };
      } else {
        console.warn('âš ï¸ Reconnaissance vocale non supportÃ©e dans ce navigateur');
        setAgentStatus('error');
      }

      // Initialiser l'audio context pour la visualisation
      initializeAudioContext();

      // Message de bienvenue SIMPLIFIÃ‰ avec dÃ©lai PLUS LONG
      setTimeout(() => {
        const welcomeMessage = "Bonjour, je suis Wally votre assistant RH. Que puis-je faire pour vous aujourd'hui ?";
        setResponseText(welcomeMessage);
        
        const welcomeChatMessage = {
          id: Date.now(),
          type: 'assistant',
          text: welcomeMessage,
          timestamp: new Date().toLocaleTimeString('fr-FR', { hour: '2-digit', minute: '2-digit' })
        };
        setChatMessages([welcomeChatMessage]);
        
        // Dire le message de bienvenue puis dÃ©marrer l'Ã©coute APRÃˆS un dÃ©lai PLUS LONG
        speakWithElevenLabs(welcomeMessage).then(() => {
          console.log('ğŸ¤ DÃ©marrage de l\'Ã©coute aprÃ¨s message de bienvenue...');
          setTimeout(() => {
            startListening();
          }, 3000); // âœ… CHANGÃ‰ : 3 secondes au lieu de 1
        });
      }, 2000); // âœ… CHANGÃ‰ : 2 secondes au lieu de 1

    } catch (error) {
      console.error('ğŸ’¥ Erreur lors de l\'initialisation:', error);
      setElevenLabsStatus('error');
      setAgentStatus('error');
    }
  }, []);

  // Initialisation de l'assistant
  useEffect(() => {
    initializeAssistant();
    aiService.initialize(RH_KNOWLEDGE_BASE);
    return () => {
      cleanup();
    };
  }, [initializeAssistant]);

  // Initialiser le contexte audio pour la visualisation
  const initializeAudioContext = () => {
    try {
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 256;
      setWaveformData(new Array(analyserRef.current.frequencyBinCount).fill(0));
    } catch (error) {
      console.warn('Contexte audio non disponible:', error);
    }
  };

  // DÃ©marrer l'animation de la forme d'onde
  const startWaveformAnimation = () => {
    if (!analyserRef.current) return;

    const updateWaveform = () => {
      if (!isListening) return;

      const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
      analyserRef.current.getByteFrequencyData(dataArray);
      
      setWaveformData(Array.from(dataArray));
      animationFrameRef.current = requestAnimationFrame(updateWaveform);
    };

    updateWaveform();
  };

  // ArrÃªter l'animation de la forme d'onde
  const stopWaveformAnimation = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }
  };

  // DÃ©marrer l'Ã©coute CORRIGÃ‰E
  const startListening = () => {
    // VÃ‰RIFICATIONS ROBUSTES pour Ã©viter l'erreur InvalidStateError
    if (!recognitionRef.current) {
      console.log('âš ï¸ Reconnaissance vocale non initialisÃ©e');
      return;
    }
    
    if (isListening) {
      console.log('âš ï¸ Ã‰coute dÃ©jÃ  en cours, impossible de redÃ©marrer');
      return;
    }
    
    if (agentStatus === 'listening') {
      console.log('âš ï¸ Agent dÃ©jÃ  en mode Ã©coute, impossible de redÃ©marrer');
      return;
    }
    
    if (isSpeaking) {
      console.log('âš ï¸ Assistant en train de parler, impossible de dÃ©marrer l\'Ã©coute');
      return;
    }
    
    try {
      console.log('ğŸ¤ DÃ©marrage de l\'Ã©coute...');
      recognitionRef.current.start();
      setAgentStatus('listening');
    } catch (error) {
      console.error('Erreur lors du dÃ©marrage:', error);
      setAgentStatus('error');
    }
  };

  // ArrÃªter l'Ã©coute CORRIGÃ‰E
  const stopListening = () => {
    if (recognitionRef.current && isListening) {
      try {
        console.log('ğŸ›‘ ArrÃªt manuel de l\'Ã©coute...');
        recognitionRef.current.stop();
        setIsListening(false);
        setAgentStatus('ready');
        setShowWaveform(false);
        stopWaveformAnimation();
        setCurrentQuery('');
      } catch (error) {
        console.error('Erreur lors de l\'arrÃªt:', error);
      }
    }
  };

  // Basculer la reconnaissance vocale
  const toggleVoiceRecognition = () => {
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  };

  // Traiter les commandes vocales CORRIGÃ‰
  const handleVoiceCommand = async (command) => {
    try {
      console.log('ğŸ§  Traitement de la commande vocale:', command);
      setAgentStatus('processing');
      setIsProcessing(true);
      
      // Ajouter la commande au chat
      const userMessage = {
        id: Date.now(),
        type: 'user',
        text: command,
        timestamp: new Date().toLocaleTimeString('fr-FR', { hour: '2-digit', minute: '2-digit' })
      };
      setChatMessages(prev => [...prev, userMessage]);
      
      // Traiter avec le service AI
      const aiResponse = await aiService.generateResponse(command, chatMessages);
      
      // Ajouter la rÃ©ponse au chat
      const assistantMessage = {
        id: Date.now() + 1,
        type: 'assistant',
        text: aiResponse,
        timestamp: new Date().toLocaleTimeString('fr-FR', { hour: '2-digit', minute: '2-digit' })
      };
      setChatMessages(prev => [...prev, assistantMessage]);
      
      // Dire la rÃ©ponse
      setResponseText(aiResponse);
      await speakWithElevenLabs(aiResponse);
      
      // IMPORTANT : RedÃ©marrer l'Ã©coute APRÃˆS un dÃ©lai PLUS LONG et seulement si pas dÃ©jÃ  en cours
      console.log('ğŸ”„ RedÃ©marrage de l\'Ã©coute aprÃ¨s rÃ©ponse...');
      setTimeout(() => {
        if (!isListening && !isSpeaking && agentStatus !== 'error') {
          startListening();
        }
      }, 4000); // âœ… CHANGÃ‰ : 4 secondes au lieu de 1
      
    } catch (error) {
      console.error('âŒ Erreur lors du traitement de la commande:', error);
      const errorMessage = "DÃ©solÃ©, j'ai rencontrÃ© une erreur. Pouvez-vous reformuler ?";
      setResponseText(errorMessage);
      
      // RedÃ©marrer l'Ã©coute mÃªme en cas d'erreur avec dÃ©lai PLUS LONG et vÃ©rifications
      setTimeout(() => {
        if (!isListening && !isSpeaking && agentStatus !== 'error') {
          startListening();
        }
      }, 4000); // âœ… CHANGÃ‰ : 4 secondes au lieu de 1
    } finally {
      setIsProcessing(false);
      setAgentStatus('ready');
    }
  };

  // Parler avec ElevenLabs ou fallback local
  const speakWithElevenLabs = async (text) => {
    return new Promise((resolve) => {
      if (elevenLabsStatus === 'connected' && voiceEnabled) {
        try {
          setIsSpeaking(true);
          setAgentStatus('speaking');
          setResponseText(text);
          
          elevenLabsService.synthesizeText(text).then(audioResult => {
            if (audioResult.success) {
              const audioURL = elevenLabsService.createAudioURL(audioResult.audioBlob);
              const audio = new Audio(audioURL);
              
              audio.onended = () => {
                setIsSpeaking(false);
                setAgentStatus('ready');
                setResponseText('');
                elevenLabsService.cleanupAudioURL(audioURL);
                resolve();
              };
              
              audio.play();
            } else {
              speakLocally(text).then(() => {
                resolve();
              });
            }
          }).catch(error => {
            console.warn('SynthÃ¨se ElevenLabs Ã©chouÃ©e, fallback local');
            speakLocally(text).then(() => {
              resolve();
            });
          });
        } catch (error) {
          console.warn('SynthÃ¨se ElevenLabs Ã©chouÃ©e, fallback local');
          speakLocally(text).then(() => {
            resolve();
          });
        }
      } else {
        speakLocally(text).then(() => {
          resolve();
        });
      }
    });
  };

  // SynthÃ¨se vocale locale (fallback)
  const speakLocally = (text) => {
    return new Promise((resolve) => {
      if ('speechSynthesis' in window) {
        const utterance = new SpeechSynthesisUtterance(text);
        utterance.lang = 'fr-FR';
        utterance.rate = 0.9;
        utterance.pitch = 1.0;
        
        utterance.onstart = () => {
          setIsSpeaking(true);
          setAgentStatus('speaking');
        };
        
        utterance.onend = () => {
          setIsSpeaking(false);
          setAgentStatus('ready');
          resolve();
        };
        
        utterance.onerror = () => {
          setIsSpeaking(false);
          setAgentStatus('ready');
          resolve();
        };
        
        speechSynthesis.speak(utterance);
      } else {
        resolve();
      }
    });
  };

  // Nettoyer les ressources
  const cleanup = () => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    if (audioContextRef.current) {
      audioContextRef.current.close();
    }
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }
  };

  // GÃ©rer le changement de mode de chat
  const handleChatModeChange = (mode) => {
    setChatMode(mode);
    if (mode === 'voice' && !isListening && agentStatus === 'ready') {
      startListening();
    } else if (mode === 'text' && isListening) {
      stopListening();
    }
  };

  // GÃ©rer la soumission du chat textuel
  const handleChatSubmit = async (e) => {
    e.preventDefault();
    if (!inputText.trim()) return;

    const userMessage = {
      id: Date.now(),
      type: 'user',
      text: inputText,
      timestamp: new Date().toLocaleTimeString('fr-FR', { hour: '2-digit', minute: '2-digit' })
    };
    setChatMessages(prev => [...prev, userMessage]);
    setInputText('');

    try {
      setIsTyping(true);
      const aiResponse = await aiService.generateResponse(inputText, chatMessages);
      
      const assistantMessage = {
        id: Date.now() + 1,
        type: 'assistant',
        text: aiResponse,
        timestamp: new Date().toLocaleTimeString('fr-FR', { hour: '2-digit', minute: '2-digit' })
      };
      setChatMessages(prev => [...prev, assistantMessage]);
    } catch (error) {
      console.error('Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse:', error);
    } finally {
      setIsTyping(false);
    }
  };

  return (
    <div className="modern-voice-assistant">
      {/* Header moderne avec effet glassmorphism */}
      <div className="assistant-header">
        <div className="header-content">
          <div className="assistant-avatar">
            <div className="avatar-container">
              <div className="avatar-ring"></div>
              <div className="avatar-core">
                <span className="avatar-icon">ğŸ¤–</span>
              </div>
            </div>
          </div>
          
          <div className="assistant-info">
            <h2 className="assistant-title">Assistant RH Moderne</h2>
            <p className="assistant-subtitle">Centre Diagnostic de Libreville</p>
            <div className="status-container">
              <div className={`status-pill ${agentStatus}`}>
                <span className="status-dot"></span>
                {agentStatus === 'ready' && 'PrÃªt'}
                {agentStatus === 'listening' && 'Ã‰coute...'}
                {agentStatus === 'processing' && 'Traitement...'}
                {agentStatus === 'speaking' && 'Parle...'}
                {agentStatus === 'error' && 'Erreur'}
              </div>
            </div>
          </div>
        </div>
        
        <button className="close-button" onClick={onClose}>
          <span className="close-icon">Ã—</span>
        </button>
      </div>

      {/* Contenu principal */}
      <div className="assistant-content">
        {/* Section de contrÃ´le ultra-moderne */}
        <div className="control-section">
          <h2 className="control-title">ğŸ¤ ContrÃ´le Vocal</h2>
          
          <div className="voice-controls">
            {/* Bouton vocal principal ultra-moderne */}
            <button
              className={`voice-button ${isListening ? 'listening' : ''}`}
              onClick={toggleVoiceRecognition}
              disabled={isProcessing || isSpeaking}
            >
              <span className="voice-icon">
                {isListening ? 'ğŸ¤' : 'ğŸ¤'}
              </span>
              <span className="voice-text">
                {isListening ? 'ArrÃªter l\'Ã©coute' : 'DÃ©marrer l\'Ã©coute'}
              </span>
            </button>

            {/* Bouton de basculement du mode */}
            <button
              className={`mode-button ${chatMode === 'voice' ? 'active' : ''}`}
              onClick={() => handleChatModeChange(chatMode === 'voice' ? 'text' : 'voice')}
            >
              {chatMode === 'voice' ? 'ğŸ“ Mode Texte' : 'ğŸ¤ Mode Vocal'}
            </button>
          </div>
        </div>

        {/* Section de visualisation de la forme d'onde */}
        {showWaveform && (
          <div className="waveform-section">
            <h3 className="waveform-title">ğŸµ ActivitÃ© Vocale</h3>
            <div className="waveform-container">
              {waveformData.map((value, index) => (
                <div
                  key={index}
                  className="waveform-bar"
                  style={{
                    height: `${(value / 255) * 100}%`,
                    backgroundColor: `hsl(${200 + (value / 255) * 60}, 70%, 60%)`
                  }}
                />
              ))}
            </div>
          </div>
        )}

        {/* Section d'affichage de la requÃªte */}
        {currentQuery && (
          <div className="query-display">
            <h3 className="query-label">ğŸ¯ RequÃªte DÃ©tectÃ©e</h3>
            <div className="query-text">{currentQuery}</div>
          </div>
        )}

        {/* Section d'affichage de la rÃ©ponse */}
        {responseText && (
          <div className="response-display">
            <h3 className="response-label">ğŸ’¬ RÃ©ponse de Wally</h3>
            <div className="response-text">{responseText}</div>
          </div>
        )}

        {/* Section de chat */}
        <div className="chat-section">
          <div className="chat-header">
            <h3>ğŸ’¬ Conversation</h3>
            <div className="chat-mode-indicator">
              Mode: {chatMode === 'voice' ? 'ğŸ¤ Vocal' : 'ğŸ“ Texte'}
            </div>
          </div>

          {/* Messages du chat */}
          <div className="chat-messages">
            {chatMessages.map((message) => (
              <div key={message.id} className={`chat-message ${message.type}`}>
                <div className="message-content">
                  <div className="message-text">{message.text}</div>
                  <div className="message-timestamp">{message.timestamp}</div>
                </div>
              </div>
            ))}
            {isTyping && (
              <div className="chat-message assistant typing">
                <div className="message-content">
                  <div className="typing-indicator">
                    <span></span>
                    <span></span>
                    <span></span>
                  </div>
                </div>
              </div>
            )}
          </div>

          {/* Input textuel pour le mode texte */}
          {chatMode === 'text' && (
            <form onSubmit={handleChatSubmit} className="chat-input-form">
              <input
                type="text"
                value={inputText}
                onChange={(e) => setInputText(e.target.value)}
                placeholder="Tapez votre message..."
                className="chat-input"
                disabled={isTyping}
              />
              <button type="submit" className="chat-submit" disabled={isTyping}>
                Envoyer
              </button>
            </form>
          )}
        </div>

        {/* Section d'aide */}
        <div className="help-section">
          <h3 className="help-title">ğŸ’¡ Comment utiliser Wally</h3>
          <div className="help-content">
            <div className="usage-tip">
              <h4>ğŸ¤ Mode Vocal</h4>
              <p>Cliquez sur le bouton microphone et parlez clairement. Wally vous Ã©coutera et rÃ©pondra vocalement.</p>
            </div>
            <div className="usage-tip">
              <h4>ğŸ“ Mode Texte</h4>
              <p>Utilisez le champ de texte pour taper vos questions et obtenir des rÃ©ponses Ã©crites.</p>
            </div>
            <div className="usage-tip">
              <h4>ğŸ”„ Conversation Continue</h4>
              <p>Wally reste en mode Ã©coute et peut traiter plusieurs commandes consÃ©cutives.</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  );
};

export default ModernVoiceAssistantFixed;
